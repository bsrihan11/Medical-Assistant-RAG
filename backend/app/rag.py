from langchain.prompts import PromptTemplate
from app.llm import initialize_rag_pipeline

llm_model, embedding_model, vector_store, retriever = initialize_rag_pipeline()





def format_messages(messages):
    """
    Formats conversation messages into a structured text format.

    Args:
        messages: A list of message objects containing 'query' and 'output'.

    Returns:
        A string representation of the conversation.
    """
    if len(messages) == 0:
        return ""
    formatted_messages = []
    for message in messages:
        formatted_messages.append('User: ' + message.question)
        formatted_messages.append('AI: ' + message.answer)

    return "\n".join(formatted_messages)


def generate_summary(messages):
    """
    Generates a summary of the conversation using the LLM.

    Args:
        llm: The LLM instance to use for summarization.
        messages: The conversation messages to summarize.

    Returns:
        The generated summary string.
    """
    
    messages_tb_summarized = format_messages(messages[:-2]) # Exclude the last two messages for summarization

    summary_prompt = PromptTemplate(
        input_variables=['messages_tb_summarized'],
        template="""
            You are an AI assistant tasked with summarizing a conversation between a human and an AI assistant.
            Please write a clear and concise summary of the following conversation. Focus on key questions, responses, decisions, and outcomes. Keep the tone professional and informative. Avoid including filler phrases or unrelated small talk.
            Limit the summary to approximately 1250 tokens or fewer.

            Conversation:
            {messages_tb_summarized}

            """
    )

    s = llm_model.invoke(summary_prompt.format(
        messages_tb_summarized=messages_tb_summarized))

    return s.content


def get_rag_reply(query, messages, long_term_memory):
    """
    Generates a RAG-based reply to a user query using memory and retrieved documents.

    Args:
        query: The user's current query.
        llm: The LLM instance.
        retriever: The document retriever.
        messages: Previous messages for short-term memory.
        long_term_summary: A long-term memory summary.

    Returns:
        A string response generated by the LLM.
    """
    prompt_template = PromptTemplate(
        input_variables=["long_term_memory",
                         "short_term_memory", "retrieved_docs", "user_query"],
        template="""
        You are a trusted, careful AI medical assistant.
        
        [User Query]
        {user_query}

        [Relevant Medical Documents]
        {retrieved_docs}

        [Long-Term Memory]
        {long_term_memory}

        [Short-Term Memory]
        {short_term_memory}

        Instructions:
        - Use only the above mentioned Medical Documents, Long-Term Memory and Short-Term Memory.
        - If unsure, refer the user to a doctor.
        - Do not mention or reference any documents, memory, context source, or prior chat turns in your answer.

        Response:
        """
    )

    retrieved_docs = retriever.get_relevant_documents(query)
    retrieved_text = "\n".join([doc.page_content for doc in retrieved_docs])


    prompt = prompt_template.format(
        long_term_memory = long_term_memory,
        short_term_memory = format_messages(messages),
        retrieved_docs = retrieved_text,
        user_query = query
    )

    ai_response = llm_model.invoke(prompt)

    return ai_response.content


def get_title(first_query):
    """
    Generates a concise 5-token title summarizing the user's first query.

    Args:
        first_query: The initial user query string.

    Returns:
        A 5-token title string generated by the LLM.
    """
    title_prompt = PromptTemplate(
        input_variables=["query"],
        template="""
        You are an AI assistant that creates concise and descriptive titles for user queries.

        Task: Generate a 5-token title (no more, no less) that clearly summarizes the user's query.
        Avoid punctuation unless absolutely necessary, and use relevant keywords.

        Query:
        {query}

        5-token Title:
        """
    )

    prompt = title_prompt.format(query=first_query)
    response = llm_model.invoke(prompt)

    return response.content.strip()
